{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1yRslbbi1hR"
      },
      "source": [
        "# Business Performance & Customer Insights Analysis\n",
        "\n",
        "**PROJECT:** Data Cleaning & Validation Phase\n",
        "**Tools:** Python (Pandas, NumPy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqYFv79Ai1hT"
      },
      "source": [
        "## Step 1: Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-Y35MNki1hV",
        "outputId": "d984afad-4e8e-4698-d301-8a3496ae1c68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "BUSINESS PERFORMANCE & CUSTOMER INSIGHTS ANALYSIS\n",
            "DATA CLEANING & VALIDATION PHASE\n",
            "================================================================================\n",
            "\n",
            "Libraries loaded successfully!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"BUSINESS PERFORMANCE & CUSTOMER INSIGHTS ANALYSIS\")\n",
        "print(\"DATA CLEANING & VALIDATION PHASE\")\n",
        "print(\"=\" * 80)\n",
        "print(\"\\nLibraries loaded successfully!\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0j7NAtdbi1hX"
      },
      "source": [
        "## Step 2: Load All Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrDBzeZai1hY",
        "outputId": "a5e28a05-9d1c-46b1-f0a7-258e152c7499"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "STEP 1: Loading Datasets\n",
            "--------------------------------------------------------------------------------\n",
            "Sales Transactions loaded: 12,550 records\n",
            "Customer Master loaded: 2,500 records\n",
            "Product Master loaded: 400 records\n",
            "Date Dimension loaded: 182 records\n",
            "\n",
            "Total datasets loaded: 4\n"
          ]
        }
      ],
      "source": [
        "print(\"=\" * 80)\n",
        "print(\"STEP 1: Loading Datasets\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "df_sales = pd.read_csv('Sales_Transactions.csv')\n",
        "print(f\"Sales Transactions loaded: {len(df_sales):,} records\")\n",
        "\n",
        "df_customers = pd.read_csv('Customer_Master.csv')\n",
        "print(f\"Customer Master loaded: {len(df_customers):,} records\")\n",
        "\n",
        "df_products = pd.read_csv('Product_Master.csv')\n",
        "print(f\"Product Master loaded: {len(df_products):,} records\")\n",
        "\n",
        "df_date = pd.read_csv('Date_Dimension.csv')\n",
        "print(f\"Date Dimension loaded: {len(df_date):,} records\")\n",
        "\n",
        "print(f\"\\nTotal datasets loaded: 4\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeVApYfLi1ha"
      },
      "source": [
        "## Step 3: Initial Data Inspection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHFqA3Cci1hc",
        "outputId": "91a9d610-7821-4e1a-dca9-18f01061541c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "STEP 2: Initial Data Inspection\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "1. SALES TRANSACTIONS STRUCTURE:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 12550 entries, 0 to 12549\n",
            "Data columns (total 15 columns):\n",
            " #   Column             Non-Null Count  Dtype  \n",
            "---  ------             --------------  -----  \n",
            " 0   transaction_id     12550 non-null  int64  \n",
            " 1   transaction_date   12550 non-null  object \n",
            " 2   customer_id        12550 non-null  int64  \n",
            " 3   product_id         12550 non-null  object \n",
            " 4   category           12550 non-null  object \n",
            " 5   quantity           12550 non-null  int64  \n",
            " 6   unit_price         12550 non-null  float64\n",
            " 7   total_amount       12550 non-null  float64\n",
            " 8   discount_applied   12550 non-null  float64\n",
            " 9   payment_method     12450 non-null  object \n",
            " 10  store_location     12550 non-null  object \n",
            " 11  salesperson_id     12550 non-null  object \n",
            " 12  profit             12550 non-null  float64\n",
            " 13  profit_margin_pct  12550 non-null  float64\n",
            " 14  data_quality_flag  12550 non-null  object \n",
            "dtypes: float64(5), int64(3), object(7)\n",
            "memory usage: 1.4+ MB\n",
            "None\n",
            "\n",
            "2. CUSTOMER MASTER STRUCTURE:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2500 entries, 0 to 2499\n",
            "Data columns (total 13 columns):\n",
            " #   Column                Non-Null Count  Dtype  \n",
            "---  ------                --------------  -----  \n",
            " 0   customer_id           2500 non-null   int64  \n",
            " 1   customer_name         2500 non-null   object \n",
            " 2   age                   2500 non-null   int64  \n",
            " 3   age_group             2500 non-null   object \n",
            " 4   gender                2500 non-null   object \n",
            " 5   city                  2500 non-null   object \n",
            " 6   state                 2500 non-null   object \n",
            " 7   customer_type         2500 non-null   object \n",
            " 8   registration_date     2500 non-null   object \n",
            " 9   email                 2375 non-null   object \n",
            " 10  phone                 2425 non-null   object \n",
            " 11  is_active             2500 non-null   bool   \n",
            " 12  total_lifetime_value  2500 non-null   float64\n",
            "dtypes: bool(1), float64(1), int64(2), object(9)\n",
            "memory usage: 236.9+ KB\n",
            "None\n",
            "\n",
            "3. PRODUCT MASTER STRUCTURE:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 400 entries, 0 to 399\n",
            "Data columns (total 9 columns):\n",
            " #   Column          Non-Null Count  Dtype  \n",
            "---  ------          --------------  -----  \n",
            " 0   product_id      400 non-null    object \n",
            " 1   product_name    400 non-null    object \n",
            " 2   category        400 non-null    object \n",
            " 3   sub_category    400 non-null    object \n",
            " 4   brand           400 non-null    object \n",
            " 5   cost_price      400 non-null    float64\n",
            " 6   selling_price   400 non-null    float64\n",
            " 7   stock_quantity  400 non-null    int64  \n",
            " 8   supplier        400 non-null    object \n",
            "dtypes: float64(2), int64(1), object(6)\n",
            "memory usage: 28.3+ KB\n",
            "None\n",
            "\n",
            "4. DATE DIMENSION STRUCTURE:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 182 entries, 0 to 181\n",
            "Data columns (total 8 columns):\n",
            " #   Column       Non-Null Count  Dtype \n",
            "---  ------       --------------  ----- \n",
            " 0   date         182 non-null    object\n",
            " 1   day_of_week  182 non-null    object\n",
            " 2   week_number  182 non-null    int64 \n",
            " 3   month        182 non-null    object\n",
            " 4   quarter      182 non-null    object\n",
            " 5   year         182 non-null    int64 \n",
            " 6   is_weekend   182 non-null    bool  \n",
            " 7   is_holiday   182 non-null    bool  \n",
            "dtypes: bool(2), int64(2), object(4)\n",
            "memory usage: 9.0+ KB\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 2: Initial Data Inspection\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "print(\"\\n1. SALES TRANSACTIONS STRUCTURE:\")\n",
        "print(df_sales.info())\n",
        "\n",
        "print(\"\\n2. CUSTOMER MASTER STRUCTURE:\")\n",
        "print(df_customers.info())\n",
        "\n",
        "print(\"\\n3. PRODUCT MASTER STRUCTURE:\")\n",
        "print(df_products.info())\n",
        "\n",
        "print(\"\\n4. DATE DIMENSION STRUCTURE:\")\n",
        "print(df_date.info())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48DAGrvUi1hh"
      },
      "source": [
        "## Step 4: Data Quality Assessment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7IfJmHNzi1hi",
        "outputId": "6124c420-42e7-4feb-a4e5-0dad12b115ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "STEP 3: Data Quality Assessment\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "MISSING VALUES ANALYSIS:\n",
            "\n",
            "Sales Transactions:\n",
            "payment_method    100\n",
            "dtype: int64\n",
            "\n",
            "Customer Master:\n",
            "email    125\n",
            "phone     75\n",
            "dtype: int64\n",
            "\n",
            "Product Master:\n",
            "No missing values\n",
            "\n",
            "DUPLICATE RECORDS ANALYSIS:\n",
            "\n",
            "Sales Transactions: 49 duplicate records found\n",
            "Customer Master: 0 duplicate customer IDs\n",
            "Product Master: 0 duplicate product IDs\n",
            "\n",
            "DATA TYPE VALIDATION:\n",
            "\n",
            "Sales - Date Column Type: object\n",
            "Customer - Registration Date Type: object\n",
            "Date Dimension - Date Type: object\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 3: Data Quality Assessment\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "print(\"\\nMISSING VALUES ANALYSIS:\")\n",
        "print(\"\\nSales Transactions:\")\n",
        "sales_missing = df_sales.isnull().sum()\n",
        "print(sales_missing[sales_missing > 0] if sales_missing.sum() > 0 else \"No missing values\")\n",
        "\n",
        "print(\"\\nCustomer Master:\")\n",
        "customer_missing = df_customers.isnull().sum()\n",
        "print(customer_missing[customer_missing > 0] if customer_missing.sum() > 0 else \"No missing values\")\n",
        "\n",
        "print(\"\\nProduct Master:\")\n",
        "product_missing = df_products.isnull().sum()\n",
        "print(product_missing[product_missing > 0] if product_missing.sum() > 0 else \"No missing values\")\n",
        "\n",
        "print(\"\\nDUPLICATE RECORDS ANALYSIS:\")\n",
        "sales_duplicates = df_sales.duplicated().sum()\n",
        "customer_duplicates = df_customers.duplicated(subset=['customer_id']).sum()\n",
        "product_duplicates = df_products.duplicated(subset=['product_id']).sum()\n",
        "\n",
        "print(f\"\\nSales Transactions: {sales_duplicates} duplicate records found\")\n",
        "print(f\"Customer Master: {customer_duplicates} duplicate customer IDs\")\n",
        "print(f\"Product Master: {product_duplicates} duplicate product IDs\")\n",
        "\n",
        "print(\"\\nDATA TYPE VALIDATION:\")\n",
        "print(\"\\nSales - Date Column Type:\", df_sales['transaction_date'].dtype)\n",
        "print(\"Customer - Registration Date Type:\", df_customers['registration_date'].dtype)\n",
        "print(\"Date Dimension - Date Type:\", df_date['date'].dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7P13zpVvi1hl"
      },
      "source": [
        "## Step 5: Clean Sales Transactions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NyYp3xZqi1hm",
        "outputId": "5d8f4313-6554-49a8-e887-c04480645573"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "STEP 4: Cleaning Sales Transactions\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "1. Removing duplicate transactions...\n",
            " - Removed 49 duplicate records\n",
            " - Clean records: 12,501\n",
            "\n",
            "2. Converting transaction_date to datetime...\n",
            " - Date column converted successfully\n",
            " - Date range: 2024-01-01 00:00:00 to 2024-06-30 00:00:00\n",
            "\n",
            "3. Handling missing payment methods...\n",
            " - Found 100 records with missing payment method\n",
            " - Filled missing values with 'Unknown'\n",
            "\n",
            "4. Validating quantity column...\n",
            " - Quantity column is already numeric\n",
            "\n",
            "5. Detecting and handling outliers in transaction amounts...\n",
            " - Found 1608 outlier transactions\n",
            " - Outlier range: INR 79,068.90 to INR 1,623,312.00\n",
            " - Outliers flagged for review (not removed)\n",
            "\n",
            "6. Validating discount percentages...\n",
            " - Invalid discounts: 0\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 4: Cleaning Sales Transactions\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "original_sales_count = len(df_sales)\n",
        "\n",
        "print(\"\\n1. Removing duplicate transactions...\")\n",
        "df_sales_clean = df_sales.drop_duplicates()\n",
        "duplicates_removed = original_sales_count - len(df_sales_clean)\n",
        "print(f\" - Removed {duplicates_removed} duplicate records\")\n",
        "print(f\" - Clean records: {len(df_sales_clean):,}\")\n",
        "\n",
        "print(\"\\n2. Converting transaction_date to datetime...\")\n",
        "df_sales_clean['transaction_date'] = pd.to_datetime(df_sales_clean['transaction_date'])\n",
        "print(\" - Date column converted successfully\")\n",
        "print(f\" - Date range: {df_sales_clean['transaction_date'].min()} to {df_sales_clean['transaction_date'].max()}\")\n",
        "\n",
        "print(\"\\n3. Handling missing payment methods...\")\n",
        "missing_payment = df_sales_clean['payment_method'].isnull().sum()\n",
        "print(f\" - Found {missing_payment} records with missing payment method\")\n",
        "\n",
        "df_sales_clean['payment_method'].fillna('Unknown', inplace=True)\n",
        "print(\" - Filled missing values with 'Unknown'\")\n",
        "\n",
        "print(\"\\n4. Validating quantity column...\")\n",
        "if df_sales_clean['quantity'].dtype == 'object':\n",
        "    print(\" - Warning: Quantity column has mixed data types\")\n",
        "    df_sales_clean['quantity'] = pd.to_numeric(df_sales_clean['quantity'], errors='coerce')\n",
        "    qty_nulls = df_sales_clean['quantity'].isnull().sum()\n",
        "    if qty_nulls > 0:\n",
        "        print(f\" - Converted {qty_nulls} invalid quantities to null\")\n",
        "        df_sales_clean['quantity'].fillna(df_sales_clean['quantity'].median(), inplace=True)\n",
        "        print(\" - Filled nulls with median quantity\")\n",
        "else:\n",
        "    print(\" - Quantity column is already numeric\")\n",
        "\n",
        "print(\"\\n5. Detecting and handling outliers in transaction amounts...\")\n",
        "Q1 = df_sales_clean['total_amount'].quantile(0.25)\n",
        "Q3 = df_sales_clean['total_amount'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "lower_bound = Q1 - 3 * IQR\n",
        "upper_bound = Q3 + 3 * IQR\n",
        "\n",
        "outliers = df_sales_clean[(df_sales_clean['total_amount'] < lower_bound) |\n",
        "                          (df_sales_clean['total_amount'] > upper_bound)]\n",
        "print(f\" - Found {len(outliers)} outlier transactions\")\n",
        "print(f\" - Outlier range: INR {outliers['total_amount'].min():,.2f} to INR {outliers['total_amount'].max():,.2f}\")\n",
        "\n",
        "df_sales_clean['is_outlier'] = ((df_sales_clean['total_amount'] < lower_bound) |\n",
        "                                 (df_sales_clean['total_amount'] > upper_bound)).astype(int)\n",
        "print(\" - Outliers flagged for review (not removed)\")\n",
        "\n",
        "print(\"\\n6. Validating discount percentages...\")\n",
        "invalid_discounts = df_sales_clean[(df_sales_clean['discount_applied'] < 0) |\n",
        "                                    (df_sales_clean['discount_applied'] > 100)]\n",
        "print(f\" - Invalid discounts: {len(invalid_discounts)}\")\n",
        "if len(invalid_discounts) > 0:\n",
        "    df_sales_clean.loc[invalid_discounts.index, 'discount_applied'] = 0\n",
        "    print(\" - Reset invalid discounts to 0\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVvu4an2i1hn"
      },
      "source": [
        "## Step 6: Clean Customer Master"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ooB-63gqi1hp",
        "outputId": "63a45307-2ed8-422c-dc98-eb1ac2c26a61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "STEP 5: Cleaning Customer Master\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "1. Checking for duplicate customer IDs...\n",
            " - Removed 0 duplicate customer records\n",
            "\n",
            "2. Standardizing city names...\n",
            " - Fixed 32 city name typos\n",
            " - Unique cities: 8\n",
            "\n",
            "3. Handling missing emails...\n",
            " - Found 125 customers without email\n",
            " - Filled missing emails with placeholder\n",
            "\n",
            "4. Handling missing phone numbers...\n",
            " - Found 75 customers without phone\n",
            " - Filled missing phones with placeholder\n",
            "\n",
            "5. Converting registration_date to datetime...\n",
            " - Date column converted successfully\n",
            "\n",
            "6. Validating age groups...\n",
            " - Found 0 age group mismatches\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 5: Cleaning Customer Master\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "original_customer_count = len(df_customers)\n",
        "\n",
        "print(\"\\n1. Checking for duplicate customer IDs...\")\n",
        "df_customers_clean = df_customers.drop_duplicates(subset=['customer_id'])\n",
        "cust_dup_removed = original_customer_count - len(df_customers_clean)\n",
        "print(f\" - Removed {cust_dup_removed} duplicate customer records\")\n",
        "\n",
        "print(\"\\n2. Standardizing city names...\")\n",
        "city_fixes = {\n",
        "    'mumbai': 'Mumbai',\n",
        "    'Mumbai ': 'Mumbai',\n",
        "    'Mmbai': 'Mumbai',\n",
        "    'delhi': 'Delhi',\n",
        "    'bangalore': 'Bangalore'\n",
        "}\n",
        "\n",
        "typos_fixed = 0\n",
        "for typo, correct in city_fixes.items():\n",
        "    typo_count = (df_customers_clean['city'] == typo).sum()\n",
        "    df_customers_clean['city'] = df_customers_clean['city'].replace(typo, correct)\n",
        "    typos_fixed += typo_count\n",
        "\n",
        "print(f\" - Fixed {typos_fixed} city name typos\")\n",
        "print(f\" - Unique cities: {df_customers_clean['city'].nunique()}\")\n",
        "\n",
        "print(\"\\n3. Handling missing emails...\")\n",
        "missing_emails = df_customers_clean['email'].isnull().sum()\n",
        "print(f\" - Found {missing_emails} customers without email\")\n",
        "df_customers_clean['email'].fillna('email_not_provided@unknown.com', inplace=True)\n",
        "print(\" - Filled missing emails with placeholder\")\n",
        "\n",
        "print(\"\\n4. Handling missing phone numbers...\")\n",
        "missing_phones = df_customers_clean['phone'].isnull().sum()\n",
        "print(f\" - Found {missing_phones} customers without phone\")\n",
        "df_customers_clean['phone'].fillna('Phone Not Available', inplace=True)\n",
        "print(\" - Filled missing phones with placeholder\")\n",
        "\n",
        "print(\"\\n5. Converting registration_date to datetime...\")\n",
        "df_customers_clean['registration_date'] = pd.to_datetime(df_customers_clean['registration_date'])\n",
        "print(\" - Date column converted successfully\")\n",
        "\n",
        "print(\"\\n6. Validating age groups...\")\n",
        "def validate_age_group(row):\n",
        "    age = row['age']\n",
        "    if age < 26: return \"18-25\"\n",
        "    elif age < 36: return \"26-35\"\n",
        "    elif age < 46: return \"36-45\"\n",
        "    elif age < 61: return \"46-60\"\n",
        "    else: return \"60+\"\n",
        "\n",
        "df_customers_clean['correct_age_group'] = df_customers_clean.apply(validate_age_group, axis=1)\n",
        "mismatches = (df_customers_clean['age_group'] != df_customers_clean['correct_age_group']).sum()\n",
        "print(f\" - Found {mismatches} age group mismatches\")\n",
        "if mismatches > 0:\n",
        "    df_customers_clean['age_group'] = df_customers_clean['correct_age_group']\n",
        "    print(\" - Corrected age groups\")\n",
        "df_customers_clean.drop('correct_age_group', axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4v2_29ai1hr"
      },
      "source": [
        "## Step 7: Clean Product Master"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0J33I_Ji1hr",
        "outputId": "8f2b1882-caa3-40ce-ca85-80963e02a22b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "STEP 6: Cleaning Product Master\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "1. Checking for duplicate products...\n",
            " - Removed 0 duplicate product records\n",
            "\n",
            "2. Validating pricing logic (cost_price <= selling_price)...\n",
            " - Found 0 products with cost > selling price\n",
            "\n",
            "3. Validating stock quantities...\n",
            " - Found 0 products with negative stock\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 6: Cleaning Product Master\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "print(\"\\n1. Checking for duplicate products...\")\n",
        "df_products_clean = df_products.drop_duplicates(subset=['product_id'])\n",
        "prod_dup_removed = len(df_products) - len(df_products_clean)\n",
        "print(f\" - Removed {prod_dup_removed} duplicate product records\")\n",
        "\n",
        "print(\"\\n2. Validating pricing logic (cost_price <= selling_price)...\")\n",
        "invalid_pricing = df_products_clean[df_products_clean['cost_price'] > df_products_clean['selling_price']]\n",
        "print(f\" - Found {len(invalid_pricing)} products with cost > selling price\")\n",
        "if len(invalid_pricing) > 0:\n",
        "    df_products_clean.loc[invalid_pricing.index, ['cost_price', 'selling_price']] = \\\n",
        "        df_products_clean.loc[invalid_pricing.index, ['selling_price', 'cost_price']].values\n",
        "    print(f\" - Corrected pricing for {len(invalid_pricing)} products\")\n",
        "\n",
        "print(\"\\n3. Validating stock quantities...\")\n",
        "negative_stock = df_products_clean[df_products_clean['stock_quantity'] < 0]\n",
        "print(f\" - Found {len(negative_stock)} products with negative stock\")\n",
        "if len(negative_stock) > 0:\n",
        "    df_products_clean.loc[negative_stock.index, 'stock_quantity'] = 0\n",
        "    print(\" - Reset negative stock to 0\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTfplrTgi1hs"
      },
      "source": [
        "## Step 8: Clean Date Dimension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftw6gw6ri1hs",
        "outputId": "242cb97f-e376-45b5-83c6-7abbe6e3b778"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "STEP 7: Cleaning Date Dimension\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "1. Converting date column to datetime...\n",
            " - Date column converted successfully\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 7: Cleaning Date Dimension\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "print(\"\\n1. Converting date column to datetime...\")\n",
        "df_date_clean = df_date.copy()\n",
        "df_date_clean['date'] = pd.to_datetime(df_date_clean['date'])\n",
        "print(\" - Date column converted successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEy48XPWi1ht"
      },
      "source": [
        "## Step 9: Cross-Table Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3-Rn6Xvi1ht",
        "outputId": "d5caaa96-5cfc-4665-c148-a4c7b7efff0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "STEP 8: Cross-Table Validation (Data Integrity)\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "1. Checking customer_id integrity...\n",
            " - Found 0 orphan customer IDs in sales\n",
            "\n",
            "2. Checking product_id integrity...\n",
            " - Found 0 orphan product IDs in sales\n",
            "\n",
            "3. Checking transaction date coverage...\n",
            " - Transaction date range: 2024-01-01 to 2024-06-30\n",
            " - Date dimension range: 2024-01-01 to 2024-06-30\n",
            " - All transaction dates covered by date dimension\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 8: Cross-Table Validation (Data Integrity)\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "print(\"\\n1. Checking customer_id integrity...\")\n",
        "valid_customer_ids = set(df_customers_clean['customer_id'])\n",
        "sales_customer_ids = set(df_sales_clean['customer_id'])\n",
        "orphan_customers = sales_customer_ids - valid_customer_ids\n",
        "print(f\" - Found {len(orphan_customers)} orphan customer IDs in sales\")\n",
        "if len(orphan_customers) > 0:\n",
        "    print(f\" - Warning: {len(orphan_customers)} transactions reference non-existent customers\")\n",
        "\n",
        "print(\"\\n2. Checking product_id integrity...\")\n",
        "valid_product_ids = set(df_products_clean['product_id'])\n",
        "sales_product_ids = set(df_sales_clean['product_id'])\n",
        "orphan_products = sales_product_ids - valid_product_ids\n",
        "print(f\" - Found {len(orphan_products)} orphan product IDs in sales\")\n",
        "if len(orphan_products) > 0:\n",
        "    print(f\" - Warning: {len(orphan_products)} transactions reference non-existent products\")\n",
        "\n",
        "print(\"\\n3. Checking transaction date coverage...\")\n",
        "min_txn_date = df_sales_clean['transaction_date'].min()\n",
        "max_txn_date = df_sales_clean['transaction_date'].max()\n",
        "min_dim_date = df_date_clean['date'].min()\n",
        "max_dim_date = df_date_clean['date'].max()\n",
        "\n",
        "print(f\" - Transaction date range: {min_txn_date.date()} to {max_txn_date.date()}\")\n",
        "print(f\" - Date dimension range: {min_dim_date.date()} to {max_dim_date.date()}\")\n",
        "\n",
        "if min_txn_date >= min_dim_date and max_txn_date <= max_dim_date:\n",
        "    print(\" - All transaction dates covered by date dimension\")\n",
        "else:\n",
        "    print(\" - Warning: Some transaction dates outside date dimension range\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWa0GJA2i1hv"
      },
      "source": [
        "## Step 10: Final Data Quality Summary & Export"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yeZyneCZi1hw",
        "outputId": "737d256e-f602-456d-e666-493670d85f8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "DATA CLEANING SUMMARY\n",
            "================================================================================\n",
            "\n",
            "BEFORE vs AFTER CLEANING:\n",
            "\n",
            "Sales Transactions:\n",
            " - Original records: 12,550\n",
            " - Clean records: 12,501\n",
            " - Removed: 49\n",
            "\n",
            "Customer Master:\n",
            " - Original records: 2,500\n",
            " - Clean records: 2,500\n",
            " - Removed: 0\n",
            "\n",
            "Product Master:\n",
            " - Original records: 400\n",
            " - Clean records: 400\n",
            " - Removed: 0\n",
            "\n",
            "DATA QUALITY IMPROVEMENTS:\n",
            " - Duplicates removed from all tables\n",
            " - Missing values handled appropriately\n",
            " - Date columns converted to datetime\n",
            " - City names standardized\n",
            " - Pricing logic validated\n",
            " - Outliers flagged for analysis\n",
            " - Cross-table integrity checked\n",
            "\n",
            "================================================================================\n",
            "STEP 9: Saving Cleaned Data\n",
            "--------------------------------------------------------------------------------\n",
            "Saved Sales_Transactions_Clean.csv\n",
            "Saved Customer_Master_Clean.csv\n",
            "Saved Product_Master_Clean.csv\n",
            "Saved Date_Dimension_Clean.csv\n",
            "\n",
            "================================================================================\n",
            "DATA CLEANING COMPLETED SUCCESSFULLY\n",
            "================================================================================\n",
            "\n",
            "Cleaned datasets ready for Exploratory Data Analysis (EDA)\n",
            "- All data quality issues addressed\n",
            "- Data consistency improved\n",
            "- Reporting accuracy enhanced\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"DATA CLEANING SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\nBEFORE vs AFTER CLEANING:\")\n",
        "print(f\"\\nSales Transactions:\")\n",
        "print(f\" - Original records: {original_sales_count:,}\")\n",
        "print(f\" - Clean records: {len(df_sales_clean):,}\")\n",
        "print(f\" - Removed: {original_sales_count - len(df_sales_clean):,}\")\n",
        "\n",
        "print(f\"\\nCustomer Master:\")\n",
        "print(f\" - Original records: {original_customer_count:,}\")\n",
        "print(f\" - Clean records: {len(df_customers_clean):,}\")\n",
        "print(f\" - Removed: {original_customer_count - len(df_customers_clean):,}\")\n",
        "\n",
        "print(f\"\\nProduct Master:\")\n",
        "print(f\" - Original records: {len(df_products):,}\")\n",
        "print(f\" - Clean records: {len(df_products_clean):,}\")\n",
        "print(f\" - Removed: {len(df_products) - len(df_products_clean):,}\")\n",
        "\n",
        "print(\"\\nDATA QUALITY IMPROVEMENTS:\")\n",
        "print(\" - Duplicates removed from all tables\")\n",
        "print(\" - Missing values handled appropriately\")\n",
        "print(\" - Date columns converted to datetime\")\n",
        "print(\" - City names standardized\")\n",
        "print(\" - Pricing logic validated\")\n",
        "print(\" - Outliers flagged for analysis\")\n",
        "print(\" - Cross-table integrity checked\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 9: Saving Cleaned Data\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "df_sales_clean.to_csv('Sales_Transactions_Clean.csv', index=False)\n",
        "print(\"Saved Sales_Transactions_Clean.csv\")\n",
        "\n",
        "df_customers_clean.to_csv('Customer_Master_Clean.csv', index=False)\n",
        "print(\"Saved Customer_Master_Clean.csv\")\n",
        "\n",
        "df_products_clean.to_csv('Product_Master_Clean.csv', index=False)\n",
        "print(\"Saved Product_Master_Clean.csv\")\n",
        "\n",
        "df_date_clean.to_csv('Date_Dimension_Clean.csv', index=False)\n",
        "print(\"Saved Date_Dimension_Clean.csv\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"DATA CLEANING COMPLETED SUCCESSFULLY\")\n",
        "print(\"=\" * 80)\n",
        "print(\"\\nCleaned datasets ready for Exploratory Data Analysis (EDA)\")\n",
        "print(\"- All data quality issues addressed\")\n",
        "print(\"- Data consistency improved\")\n",
        "print(\"- Reporting accuracy enhanced\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
